kubernates-command
   # kubectl create -f <deploymentfile/svc/configmap/secret/pv/pvc>
   # kubectl get deployment/svc/configmap/secret/pv/pvc/pods
   # kubectl describe pod <pod name>
   # kubectl get pod -o wide
   # kubectl exec -it <pod_name> -- /bin/bash # go inside pod
   # kubectl logs <pod_name>


Docker:
    # docker run -d mysql:latest  # create container in detach mode
    # docker run -d --name mysql -p 3306:3306 mysql:latest
    # docker images # show all images 
    # docker ps # show all running container
    # docker ps -a # show all stop and running container
    # docker run <container-id> # running in foreground
    # docker build -t <dockerhub_id/image_name>:tag . # it will create custom image from dockerfile
    # docker run <image_name> # it will download image and create and run container.
    # docker pull <image_name> # it will pull image
    # docker exec -it <container_id> /bin/bash # go inside container
    # docker run -d -v /opt/path:/var/lib/mysql mysql:latest # persistent volume
    # docker run -it --name <container_name> <image_name> # Create container and go inside the container
    # docker logs <container_name/id> # To check container logs.
    # docker stop container_name_or_id  # stop the container 
    # docker rm container_name_or_id# remive container
    # docker rmi <image_id> # remove image
   # docker inspect container_name_or_id  # Inspect Container Details

ShellScripts:
................
# /bin/bash file.sh
#./file.sh
#!/bin/bash (or) sh.dash
chmod u+x,g-x,o-x filename 
cat /etc/passwd/
chmod 777 filename
chmod +x filename 
.......
chmod u+x filename 
......
chmod= which user and which group and other 
421= read write execute
top command = which process we have running which process talking less memory and more memory......
..........
cpu = nproc 
......
ram = free
....
df = disk space usage 
.....
nproc= print the number processing untis avalible (cpu)
......
free = used the memory in the system 
......
set -x = debug mode for beging shell scripts
......
set -e exit the script when there is an error ..
......
set -o pipefail....
......
ps -ef = check or print the all  prosess id 
......
ps -ef | grep "amazon"

grep= matching patterns file
....
vim test.sh 
12
11.
1
./test.sh | grep 1 
output of the first command to the secound command ...
date | echo "this" 
this is print ...date is a system deafult command ...

Awk command use in specfic retrive in specific colum ..
..........
grep name test
    retrive
ps -ef | grep amazon | awk -f""'{print $2}'
find / slags = everything 
.........
find comand ..find the file
.............
cat logfile | grep error 
logfile storage in google storage and amazon storge ,s3 
curl command 
............
retrive the informatin from internet ...
curl https//with copy the logsfile....| grep error 
postman =api request 
........
python = request module
.......
wget command 
.........
wget _> download -> grep command on downloaded 
download the logfile...

vim ifelse.sh 
a=4
b=10
if [expression]
if [$a>$b]
   then  sdfkkfkk
      dfdggggkd
then 
   echo "a is greater than b"
else ddkkkdkkdkd
else 
echo "b is greater than a"
fi 

#iterations ->for 
for i in {1.100}; do echo $1: done 
for
do increment 
done 
trap command 
...........
trap the signal ........
trap "echo dont use the ctrl+c"sigint
kill -9 java 
.............
sig .. kill command 
ls -ltr ---> long list with timestamp and reverse 
ln -s ----> 
vim /etc/hostname
hostname centos7.devops.in
ls -lrt /etc/
What command is used to find file type
FILE<filepath>
ln -s <orignalFilePath> <LinkFilePath> command is used to create soft link.
true

FILTER AND REDIRECTION COMMAND
.................................
GREP command to find the text from the files
uptime 
cat /etc/passwd
uptime > /tmp/sysinfo.txt
cat /tmp/sysinfo.txt (output the files)
ls 
ls > /tmp/sysinfo.txt
uptime >> /tmp/sysinfo.txt (updended)
cat > /tmp/sysinfo.txt (see the other content)
free -m (memory utilization)
df -h (this is goona show you hard disk partion)
echo "good morning"
echo "############" > /tmp/sysyinfo.txt
date > /tmp/sysinfo.txt
echo "###########" >> /tmp/sysinfo.txt
uptime >> /tmp/sysinfo.txt (output the command)
echo "###########" >> /tmp/sysinfo.txt
free -m >> /tmp/sysinfo.txt (output the command)
echo "###########" >> /tmp/sysinfo.txt
df -h >> /tmp/sysinfo.txt (output the command)
echo "###########" >> /tmp/sysinfo.txt
cat /tmp/sysinfo.txt (content of the file)
this is called output redirction...........

/dev/null (doesnt content anything)
yum install vim -y (lot of output)
yum install vim -y > /dev/null 
cat /dev/null (there is nothing is there)
cat /dev/null > /tmp/sysinfo.txt (nothing file)
cat /tmp/sysinfo.txt (every thing is wiftout clean)
free -m > /dev/null
freeeee -m > /dev/null 
freeeee -m 2>> /tmp//error.log (2 means stsndard error)
free -m 1>> /tmp//error.log
(1 output by defult)
cat /tmp/error.log
if you any kind of redirediction use to (&&)
free -m &>> /tmp//error.log
freesdsd -m &>> /tmp//error.log
cat /tmp/error.log

LOG FILE
..........
cd /var/log/
ls
wc -l /etc/passwd (counting lines)
wc -l < /etc/passwd 
cd /etc
ls (count no of files)
ls | wc -l (ls command will be generated the output this input goes to write hand side )
ls | grep host

tail -20 /var/log/messages | grep -i vagrant
free -m | grep mem
free -m | grep ram
ls -l | tail
ls -l | head
find /etc -name host*

locate --> find 
updatedb   

QUESTIONS
.........
output redirection symbol... (>)
input redirection symbol ....(<)
how to append output redirection to a file 
<< filepath
is to append starnd output to a file
false
is to redirect & append all the output to file (output& error both)


USER & GROUP
............
# users and groups are used to control access to files and resources
# users login to the system by supplying their username and password
# Every file on the system is owned and group affilation and can only access the resou
its owner or group can access
# Every user of the system is assigned A unique uesr ID number (the UID)
# Users name and UID are stored in /etc/passwd
# Users are assigned a home directory and a program that is run when they login
# Users cannot read ,write or execute each others files without permission..

TYPE OF USER
.............
type   EXAMPLE  userID   GroupID      HomeDIR         SHEll
....  .......   ......   .......       .......         ....
ROOT   root     0           0            /root          /bin/bash
regular  jitu   1000 to 6000) (1000 to 6000 /home/      /bin/bash
                                           username 
SERVICE   FTP,SSH 1 to 999     1to 999    /var/ftp etc   /sbin/nologin
          apache 
 

cat /etc/passwd 
head -1 /etc/passwd
root:X:0:0:root:/root:/bin/bash
user shadow userid groupid comment homedirectoryof root user loginshell..

shadow file will be passwd incrypted...
grep vagrant /etc/passwd 
id (user name)
id vagrant...
useradd ansible
useradd jenkins
useradd aws
useradd linux
tail -4 /etc/passwd
tail -4 /etc/gruop
gruopadd devops
usermod -aG devops ansible
vim /etc/group
id aws
su -jitu
lsof -u vagrant
yum install lsof -y 
losf -u (list all the open file)
userdel -r jenkins
ls /home/
groupdel devops
How to reset password of a user ?
ans - root user can reset passwd username use only 
user can reset own passwd running just passwd......
How to switch between users in Linux cli?

su -username
Users password is stored(encrypted) in /etc/shadow file.

true.........

FILEPERMISSION
..............
text file,,link file,,...
file permission may be using ls -l
ls -l /bin/login
four symbols are used when displaying permissions
R W X NO PERMISSION
R => read permission
w => write perission
x => exucute 

- => filetype
rw - => User
--- => group
--- => others

mkdir /opt/devopsdir
ls -l /opt/
groupadd devops
useradd ansible
useradd jenkins
useradd aws
vim /etc/group  edit the group file directery ...
id ansible 
ls -ld /opt/devopsdir
chown username:groupname /opt/devopsdir (owner ship change)
chown -R useing recursible 
chmod o-r /opt/devopsdir
chmod o-w /opt/devopsdir
chmod g+w /opt/dev (write permission to group)
chmod 770 /opt/webdata (path)
chmod +x filepath command will give execute permission to all users on the filepath.

QUESTIONS
'''''''''
If normal user wants to run root commands and Admin cannot share root password as per compliance.
What should you do?
ask to admin to add your username suderfile ....

How to install single package in RedHat & Debian OS?
rpm -i packagename in redhat os
dpkg -i packagename in debian os

Where are yum repos and apt repos files located?
/etc/yum.repos.d/ for yum
/etc/apt/sources.list & /etc/apt/sources.list.d for apt

What is the command to  download a file from internet or over a network.
wget URL 
curl URL -o outputfile....

SERVICE
,,,,,,,,
How to check if a service is enabled?
systemctl is -enabled servicename
How to make sure the service is running and comes up after reboot.
first command will start and secound will enable the service at the boot time 
1. systemctl start servicename
2. systemctl enable servicename 
How to check all the process with their parent process id?
ps -ef
kill -9 PID command is to stop process forcefully and kill PID  is to stop process gracefully, child processes also will be stopped if parent process is stopped gracefully.

Difference between Zombie & Orphan Process

zombie is nothing but --> completed without waiting 
orphan is nothing but ---> completd task is still its show any process table 

ARCHIVE
.......
what is archive inlinux?
Archive files are used to collect multiple data files together into a single file for easier portability and storage, or
simply to compress files to use less storage space.

 cd /var/log 
tar -czvf (create compress verbose file)
tar -czvf jenkins_06122020.tar.gz jenkins
file tar -czvf jenkins_06122020.tar.gz (ganzib tar data)
tar -xvzf extract

NETWORK
,.......
ROUTER --> A ROUTER recives and send data on computer networks. router are sometimes confused 
with network hubs,modems, or networkswwitches However routers can combin the multyple networks 
that is final resources........
////////////////////
subnet mask: does to network range.
.............
1st ip address ----> network ip
192.168.0.0
192.168.0.1
192.168.0.2
192.168.0.254
192.168.0.255--->
last ip address ----> Broad cast 
network range... 172.20.0./16
subnets...> 172.20.1.0/24
cidr...
...........
255.0.0.0
1111111.00000000.0000000.00000000
255.255.0.0.
1111111.1111111.0000000.0000000.00000000

 

SUBNET
///////
subnet for database server 
subnet for webserver 
subnet for project ...
IPADDRESS
.........
192.168.100.1 ----> 1ST octet 2nd octtet 3rd octet 4th octet
8bits+8+8=32 bits.........
every octet start from ZERO......
0.0.0.0--255.255.255.255
00000000.00000000.00000000.00000000.(0.0.0.0)
11111111.11111111.11111111.11111111(255.255.255.255)

public IP------> for clould provider 
PROTOCOL
///////
in the networking and communictions area a protocol is the formal specification 
that defines the procedures that must be follwed when transmitting or reciveing 
data protocol define the format 
timing sequence and error checking used on the network......


OSI LAYER MODEL
///////////////
Application layer ....>  telnet ,ftp ,dhcp ,tftp ,http ,smtp ,dns, snmp
presentation layer ....>
session layer .....> 
transport layer ....> tcp transport udp
network layer ......>internet ICMP ARP RARP IP
physical layer ......> Network interface
datalink layer.......

NETWORK COMMANDS   
................
ping ipAdress ...........

tracert www.google.in

vagrant ssh web01..

netstat -antp--->showing all the TCP open port/.........

ps -ef | grep apache2 
netstat -antp | grep 3336 
ss -tunlp ..
dig www.google.com ---->To make a DNS lookup: Enter domain name (trailing 
dot will be auto-appended). Lookup and enjoy the output.

nslookup www.google.com ----> 
route -n 
The route command is the interface used to access the Linux kernel's routing tables. The route command operates in three modes: display, add, and delete. When used to add or delete routes from the routing table, 

its use is generally limited to adding and removing static route entries.

ARP
//////
The arp command displays and modifies the Internet-to-adapter address translation tables used by the Address in Networks and communication management. 
The arp command displays the current ARP entry for the host specified by the HostName variable.
MTR ---> 
.......
The mtr(my traceroute) command is a robust network diagnostic tool combining the functionality of the ping and traceroute commands
VARIABLE
/////////
$0 is the name of the script.
$1-$9 are the command line arguments.
What is the way to print exit status of last command?
echo$?
\$ ---> special meaning of character

() -----> pranthisis........
{} -----> collibrase .......

Docker
//////////////
to host our apps we need infrastructure
we use vms/cloud computing to set up infra
we isolate our service in OS of VM..
becuse of isolation we end up setting up multiple vms/instances
VMs/instances will be overprovisioned
rsult in high cap ex and op =ex ...like regular maintaince,cost, collingpower,
adminteam, operations///
evry vm has os...
VM are portable but bulky
all are isolated 
isolating services are IMP (Need OS)
High availability achived by multiple instances/VMs
portabily Matters or Eases the Deployment....

ISOLATION WITHOUT OS
////////////////////
imagine multiple services running in same os but isolated.....

CONTAINERS
.............
process running in a directory
ipaddrss of the virtual machine 


containers will be Kernal trick....
but this process will be isolated...
A directory Namespace , group 
Necessary bi/lib/ in the Directory...tomcat and jdk need in this directory


Every container that is running in the operating systems 
all this containers are going to share the operating systems kernal and kernal will be
provide the right resources to the process.

container treat like a Packge....
need 
code dependecies.........

 VM VS CONTAINER
...................
container offer isolation not virtualization
containers are OS virtuallization
VMs are Hardware virtuallization
vm need os
contaieres dont need os.
container uses Host os for compute.
resurce

what is docker
.............
container run time inverrnment 
manages your containers.....

docker engine ...> 
.....................
lots of automation will be done by docker engine 

docker engine is a damean we can connect with RESTapi ..
and client docker CLI......

manage the conainer and image and network and data volumes.....
main importnt is images pull images run containers..........























































































AWS(application archtecture} ...
////////////////////////
presntation tier = user interface (fronted) facebook.google..youtube
...........
application tier (backend ) webserver LOAD willbe increase in backend server 
............
data tier (database server) 
.........
load balancer= distribute the multyple server
...............
(USER)CLIENT->INTERNET->LOADBALANCER->SERVERS
...............
              domain name-> IP-> = DNS mapping every project has DNS configuration
ROUNDROBIN ALAGARYTHEM= 1st reqest 1st server 2nd requset 2ndserver to divert the traffic ALL the load balancer used in roundrobbin techinqby deafult
......................
sticktational alagarythem ..1st and 2nd and 3rd requset will go to 1st server that is called 
.........................
why application server runing in the multyple servr becuse of = to handle the traffic


clould computing 
.................
delivery and demanded it resources over the internet rent the software
infrastructure= database , computers, servers,loadbalancing .dns

datacenter = server 
..................

CLOUD PROVIDER 
.................
aws(amazon) ,azure(microsoft),google (gcp),salesforce (salesforce crm)
IBMcloud, oracle ,alibaba cloud, vm cloud...thease all are clould platform in the market
CLOULD computing adavantage.
...........................
pay as you go= howmuch you use u will pay
low cost 
scalability=increse server scale up/down
avalibility=24 hour avilble
reliability=strong 
security=100percent
unlimited storage=
backup= data save

EC2= elasitc clould coumpte is nothing but create unlimtied vm
...........
s3 = unlimited storage 
.......
amazons3-> bucket>>>>folder>>>object>>>public acess
s3 standard-purpose storage of frequently acceseddata fast access & object replication in multy zone
s3 IA---> interfrequent access --> long lived but less requently accesd data slow access object replication in multy AZ
s3 one zone-IA --> slow access no object replication 
s3 intelligent tiering--->> autometically moves data to most cost effective tier..
s3 glasser ...> slow dat archive
s3 glacier--> lowest cost 12 hour
s3 charges--->> storge,request,tiers,data transfer,region replications..
s3 is a global services
s3 enable --> recover the data
object bydeafult private

RDS = relatational data base service mysql
.........
dynomo DB.mango DB = no sql data base 
...........

route53= domain name mapping with ip addres by using route 53.u
.........

EBS = (elastic block store)if you store the data in ec2

......
by default EBS root volume will be attached to e ec2 instance 
linux gives you =8gb 
windows = 30 gb 
EBS= gives you 16 tb 
NOTE: we can increase volume size based on demand
VPC =(VIRTUAL PRIVATE CLOULD)
............................
vpc provides network for ec2 instance all the service behind a private network is called VPC
to connect with EC2 instance we need network for that
LBR
........ loadbalancing used in distribute into multyple server
AUTOscale= loadbalancing purpose 
...............
IAM = identity access management (who can access which service)
.........
Beanstalk = whatevr the infrastructure setup mannualy by use beanstalk to automate
.........
clouldwatch =moniter puropose 
...........
lightsail = deploy applications easly 
.........
SNS = simple notification service 
......

ON PREMISE vs CLOUD
...................
ONpremise is nothing but managed by people it is very costlyy
AMI = amazon machine image 
.........
it provides templatates to launch virtual machine 
EX= windows AMI,AMzon linux ami.ubuntuami,redhat ami,Mac ami
security GROUP
.............
security group are used to allow incoming and outgoing trafic
Inbound rules will maintain incoming traffic rules.
outbound rules will maintain outgoing traffic rules.
windows---->RDP:3389
LINUX------>ssh:22
http-------> http:80
https------->https:443
tomcat------> 8080
KEY-PAIR
.......
key-pair is kind of lock secure our ec2 instance 
key-pair is used to connect with ec2 instance securely
key-pair combination of public -key and privatekey
aws will store public key and it will provide private key for us
using public key and private key we can connect with ec2 instance securely


EC2=vm ----> KEY-PAIR
       ---->security group
       _---->vpc default
       ----->EBS defult
       ------> AMI defult
data center
............
avalibility zone is nothing but data center

EKS
....
eks stands for elastic k8s services
eks is a fully managed aws service
eks is the best place in k8s application becz security,reliability, and scalability
eks can be intregated with other AWS services such as ELB,cloudwatch,Autoscaling,IAM,and vpc
EKS makes it easy to run k8s controllplane across three avalibility zone in order to ensure highavalibility
and it autometically detect and replaces unhelthy masters
aws will have complete control over control plane. we dont have control on control plane
we node to create worker nodes and attach to controll plane
NOTE:we will create worker Nodes group using ASG group
controll plane charges + worker nodde chareges (based on instance type& no of instance)


PRE-Requisites
.............
AWS aacount with admin privilige
instance to manage/access EKS cluster using kubectl
AWS CLI access to use kubectl utility

kubectl(k8s-client-vm) ----> control (aws EKS) plane----->node1 and node2
STEPS TO CREATE EKS CLUSTER IN AWS 
...................................
STEP-1=create vpc using cloud formation (with below s3 url)
URL...
stack Name ..xyz
.................
STEP-2=crate IAM role in AWS
.....................
Entity type: AWS service
select usecase as EKS-->EKS cluster
role name --
STEP-3=cretae EKS cluster using created VPC and IAM role
cluster endpoint access: public & private
step=4 cretate ec2 instance k8s_client_machine
///////////////////////////
Elb (elastik load balancer)
////////////////////////////
distributed incoming application or network traffic across multyple targets such as amazon ec2 instances,containers,ip adress
in multyple availble zone..
1..APPLICATION LOAD BALANCER
web application https,http 7th layr
2. NETWORK LOAD BALANCER 
very exxpensive 4th layer open systems (osi) model \
it can handle the millions of request per secound 
3. CLASSIC LOAD BALANCER
the route traffic based on either application or network level information (0si)
////////////
CLOUD WATCH
///////////
monitor perfomance of aws envirnment -standard infrastructre metrics
metrics allows---> ebs,ec2,elb,route53,health checks rds amazons3 cloud front etc.........
alaram will help sns email notification 
EC2 instance--->create alaram-->amazon cloldwatch--> alaram--> alaram triggerd ---> email notifications (SNS)
genertic metrics--> collective matrics--> cpu utiliation
///////////
subnet mask: does to network range.

///////////
GIT bash COMMANDS 
..............
we are having several git commands to perform operetions with git repo
git help ....
git add .working directory to staging area 
.......
git commit -m "xjjjdk" staging area to local repo 
.............
git push --local repo to remote repo 
.........
git init - it is used to create empty repository or re-initialize existing repo
.........
clone a repo - git clone
..........
create a branch - git branch branch name
..........
switch to branh - git checkout branch name 

git status - this command will display status of current repository
...........
staged files 
...........
files which are added and they are ready to commit 
thies file name will be green colour 

un-staged files
...............
modified files to the local  will be displayed here we need to stage these files to commit.
these files will be displyaed in red colour
un-tracked files   
..............
-newly created files we need to stage them to commit 
these files will be displayed in red colour..

git pull .. to take the code from central repo
rm
git merge
git rebase 
git stash
git add --a -> to add all files at a time 
........
git rm --cached *
..........
git rm -> this command is used to un-stage newly created files
.....
git rm --cached <file name>
........
git remote add <repo-url> (this requires only first time)
..............

git push -u origin master (this is used to move changes from local to central) 
..........................

git reset -> it is used to unstage a file
.........
synatx : git reset HEAD <file-name>

checkout-UNDO
............
git checkout : it is used to discard changes done in a file 
syntax: git checkout --<file name>
git log-> to check commits history we will use git log command 
........
commit-id 
author 
timestamp 
commit msg

git clone 
..........
to take exisiting project from repository to local system we will use git clone command
syntax : git clone <repo-URl>

GIT STASH
........

it is used to record current changes and make working tree clean 
git stash is a TEMPORARY storage ..

working tree ----> his-12 manger assign a task 
the middle man comes to picture git stash 
the working tree will be clean ...
after 12 are completed and ommit and  push it 3 pm.

------> his 10  8am 

-------> 12:12 pm 20 files modifies 

git add --a
git commit -m 
git push
git stash apply 
git stash clear
git stash list 
git stash show 
git stash drop...

WHAT IS BRANCH in github
..............
when we create a git repository by defult it will be provide master branch 
----> branch are nothing but code bases 
----> we can create several branches in git repository
-----> generally in git repository we will create branches like below 
- master (defult)
-devolp ----> on-going dev team, bug fixing team, prod support dev team 
--feature --> story code only vertx code 
--QA
--UAT
-Relese --> deploy into the code relese only 
-prod ...
BRANCH MERGE 
............
feature branches merge into the master branch  we use a cocept call pull request

WORKFLOW 
.........
--> login into reposiotry
---> create devlop branch from master branch
---> clone devlop branch code 
NOTE: 
...... 
if we exucute git clone <repo-url> it clones master branch code by defult
-> if we want to clone specific branch code then we should execute below command
--> git clone  -b <brnch name> <repo-url>

GIT merge
.........
git merge is nothing but one branch to another branch ..feature branch to master branch
--> make changes and push your changes to branch which you created to featurre
--> once your devlopment & unit testing is completed then merge your changes to main branch
--> to merege changes from one branch to another branch we will create "pull Request"
---> source branch feture and target branch is master 
---> when we execute pull request, GIT HUB compare source branch and Target branch
and it will confirm can we merge these branches or not
---> if status is abale to merege then execute pull request and merge changes
---> After pull request execution got completed, we can delete that new branch which 
we created for our story...

REAL TIME SCENARIO
...................
two developers are working on sprint 7.3
muna and jitu are the developers
--> muna working on HIS-300 story
---> jitu working on HIS-301 story
NOTE: Latest code is avalible in devlop branch 

CONFLICT 
.........
developers has working on same file same line no conflic occure in the repository
 resolve confolict
 mark as resolved
commit merge
merge pull request 

GIT PULL
.........

git pull command will get latest changes from central repository to local repo 
(file transefering will happen here)

GIT featch
...........
git fetch command that tells your local git to retrive the latest metadata info from 
the original (yet doesnt do any file transefering) its more like just checking to see
if there are any changes available.  


MAVEN
//////
contents 
1.Build process
2.build tools
3.what is maven
4.maven phases
5.maven installation
6.pom.xml file
7.maven commands 
8.maven versions

build process
..............
source code--->>java,dotnet, etc jenerated is nothing but keep commiting changes to the source code mobile aap,web application
compile the code---->>test the code this is specially compile language like java,dotnet.c++,csharp
test--->> it wil work then will unit/intregation 
package---> out put packages in archive war,jar,exe,(windows)msi,zip etc
health checks---> code analysis findbugs security vernbility check code


bild tools =automates build process
...................................
ant,maven -java -- build file -xml format
msbuild-> microsoft build engine is a platform for building applictions

gradel--> groovy format
nant--> windows,.net platform

MAVEN phases 
..............
validate---> validate the project is correct and all necessary information is avalibale

compile---> compile the source code project
test
package--> distrubted format such as jarverify-->run any checks on results of intregration tests to ensure quality criteria 
install--> install the package into local repositry for use as a dependency in other projects locally
deploy---> done the build enviornment copies the final packages to the remote repo
sharings with other devolpers and projects 
google it build lifecycle maven...
 artifact--->Vprofile
packing-->war
version---> v2 
build or artifact--> vprofile-v2.war 

{}---> colli base
()---> promethiios

1st install --> sudo apt install openjdk-11-y
then install ---> mvn sudo apt install maven-y
google --> maven archive seardch
copy link address 
then extratect ...> tar xzvf apachen-maven-3.9.3-bin.tar.gz
ls 
 apache-maven-3.9.3/bin/mvn -version
mvn 
 apache-maven-3.9.3/bin/mvn -version
sudo mv apache-maven-3.9.3/opt/
/opt/apache-maven-3.9.3/bin/mvn -version
git clone 
mvn validate 
mvn test..
ls 
target folder 
exit 
mvn clean -->remove all things
mvn install 
ls /home/ubuntu/.m2 
ls /home/ubuntu/.m2/repository/---> all dependcyy bydefult store in ubunttu (localhome)
rm -rf /home/ubuntu/.m2/repository/*






